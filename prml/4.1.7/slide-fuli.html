<!DOCTYPE html>
<html>
  <head>
    <title>4.1.7 パーセプトロンアルゴリズム</title>
    <meta charset='utf-8'>
    <script src="html5slides/slides.js"></script>
    <link rel="stylesheet" type="text/css" href="html5slides/tstyles.css">
  </head>

  <style>
  </style>

  <body style='display: none'>
    <section class='slides layout-regular template-default'>

      <article>
        <h1 style=''>
          <span style='font-size:40pt;'>パーセプトロンアルゴリズム</span>
          <br>
          <span style='font-size:32pt;'>4.1.7</span>
        </h1>
        <p>
          naoya_t<br>
          2012.8.5 PRML復々習レーン
        </p>
      </article>
      

      <article>
        <h3>自己紹介</h3>
        <ul class="build">
		  <li>naoya_t</li>
		  <li><img src="images/keroro.jpg"></li>
		  <li><del>2009年5月からPRML読書会を主催</del><br>
			今は後ろのほうで一般参加者してます</li>
		  <li>パターン認識とか機械学習とか元々趣味で勉強していたはずなのですが…</li>
		  <li>＼８月から機械学習のお仕事始めました／</li>
		  <li>というわけでちゃんと勉強します</li>
        </ul>
      </article>
      <article>
        <h3>自己紹介（続き）</h3>
        <ul class="build">
          <li>最近面倒くさくて言ってないけど某国立大文学部卒<br>
			（※経歴詳細はFacebook等参照のこと）<br>
			当然ながらこの分野は元々素人</li>
		  <li>競技プログラミングとか時々やってます</li>
		  <li>Shibuya.lisp発起人の1人<br>
			時々ネタでSchemeコード書いてます。Gauche大好き</li>
		  <li>最近はCourseraのステマしてます<br>
			７月から <i>"<a href="https://www.coursera.org/course/qcomp">Quantum Mechanics and Quantum Computation</a>"</i> のクラスに参加中</li>
		  <li>量子力学楽しいかも</li>
		  <li>週末に謎のお題付きカラオケ（機械学習、自然言語処理、コンパイラ実装、量子力学etc.）を開催しています</li>
        </ul>
      </article>

      <article class='fill'>
		<img src="images/karaoke.jpg" width="100%">
		<p>※画像はイメージです</p>
	  </article>

      <article class='smaller'>
        <h2>余談：PRML読書会小史</h2>
		<p>
		  <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
		  時間遡行を何度繰り返してもワルプルムルの夜を倒せない…</p>
      </article>


      <article>
        <h3>PRML読書会（本レーン）</h3>
        <div class="build">
          <p>秋葉原の書泉ブックタワーでPRMLに一目惚れ。<br>
			面白そうな教科書だし<b>図版が綺麗だったので</b>衝動買い。</p>
		  <p>難しくて１人で読める気がしないので、当時住んでた墨田区の公民館で読書会を開いてみたのが第1回 (2009.5)。その後も公民館を転々としながら細々と開催。</p>
		  <p>万単位の戦闘力を持つガチ勢が次々到来。一方で初期の参加者は次々脱落。</p>
		</div>
	  </article>

      <article class='fill'><img src="images/scouter.jpg" width="100%"></article>

      <article>
        <h3>PRML読書会（本レーン）（続き）</h3>
        <div class="build">
		  <p>トピックは追っているつもりだけれど、議論と数式展開がもはや地球人の肉眼では見えない…</p>
		  <p>PRML hackathonを2回開催（＠曳舟文化センター）</p>
		  <p>感動の<a href="http://atnd.org/events/6603">最終回</a>は参加者16名。</p>
		  <p>スペシャルゲストにPRML訳者の1人@shima__shimaさん現る！</p>
		</div>
	  </article>

      <article class='fill'>
		<img src="images/yosegaki.jpg" width="100%">
		<p><br><br>
		  <div style='display:table-cell; height:460px; color:white; vertical-align:middle;'>
			本レーン最終回参加者の皆さんに、<br>
			PRML下巻表紙裏に寄せ書きしてもらいました！
		</span></p>
	  </article>

      <article>
        <h3>PRML読書会（復習レーン）</h3>
        <div class="build">
          <p>本レーンはゴールが見えてきたけれど、1周読んでみて振り返ってみると理解の粗さが目立つ。</p>
		  <p>＼もう1周しようず／</p>
		  <p>Shibuya.lispでお世話になっていたECナビ（現VOYAGE GROUP）さんの広い会議室を借りて再スタート。本レーンと交互に開催。(2010.5)</p>
		  <p>新たなガチ勢が到来し最大36人。<br>
			大臣に褒められた人、うんこ漏らしたとか言う人、etc…</p>
		  <p>but<br>
			幹事の体調不良により開催できず。<br>
			引き継ぎがちゃんとできずにフェードアウト…</p>
		</div>
	  </article>


      <article>
        <h3>PRML読書会（復々習レーン）</h3>
        <div class="build">
		  <p>復習レーンの再開を求める声が多かったがなかなか手を付けられないまま年月が過ぎ…</p>
		  <p>幹事役で回し続けるのは色々しんどい</p>
		  <p>幹事業を引き継ぐなり分散化するなりして再開しよう</p>
		  <p>というわけで</p>
		  <p>　　＼とりあえず有志でキックオフだけでもやろうず／</p>
		  <p>そんな折、DeNAさんがヒカリエに引っ越してきた</p>
		  <p>　　　＼ヒカリエでPRMLキックオフやりたい！／</p>
		  <p>とか言っていたら showyouさんのお陰で実現 (2012.5)</p>
		</div>
	  </article>

      <article>
        <h3>PRML読書会（復々習レーン）（続き）</h3>
        <div class="build">
		  <p>幹事業務も有志の皆さんが快く引き受けて下さいました：</p>
		  <table>
			<tr><td>司会進行（＆アイスブレイク）</td> <td>sleepy_yoshiさん</td></tr>
			<tr><td>会場（＆じゃんけん）</td> <td>showyouさん</td></tr>
			<tr><td>出欠＆ATND</td> <td>Prunus1350さん</td></tr>
		  </table>
		  <p>（この場を借りて御礼）</p>
		  <p>お陰様で今では最後列の席で絶賛<ruby>一般参加者<rt>ステルス</rt></ruby>しています。</p>
		  <p>それにしても今回は最初からガチ勢しか来てない…</p>
		</div>
	  </article>


      <article>
        <h2>もう１つ<ruby>余談<rt>ステマ</rt></ruby></h2>
      </article>

      <article>
        <h3>[ステマ] 皆さん、<ruby>Coursera<rt>コーセラ</rt></ruby>をご存知ですか？</h3>
        <div class="build">
		  <p>スタンフォード大CS学部の
			<ul>
			  <li>Daphne Koller教授</li>
			  <li>Andrew Ng准教授<br>
				<span style='font-size: 14pt;'>
				  ※"Ng"は[ŋ:] みたいな発音のようです。finger [fiŋgə:]のngじゃなくてsinger [siŋə:] のngで。</span>
			  </li>
			</ul>
			</p>
		  <p>が立ち上げた、Stanford他17大学116講義（※2012/8/5現在）をオンラインで無料で提供するベンチャー企業。</p>
		  <p align="center"><a href="https://www.coursera.org/">https://www.coursera.org/</a></p>
		  <p>現時点では提供されている講義はCSを中心とした科学系が大半だが、今後人文系の講義も増やしていくとのこと。</p>
		</div>
	  </article>

      <article class='fill'><img src="images/coursera_top.png" width="100%"></article>

      <article>
        <h3>Courseraの特長</h3>
        <ul class="build">
		  <li>1本1本の講義ビデオが短め（10分前後）に出来ていて、集中力が途切れにくい</li>
		  <li>講義は基本的に英語だが、クローズドキャプション（英語字幕）をON/OFFできるし、速度も調整可能</li>
		  <li>途中に理解を試すクイズがあったりするので飽きないし落ちこぼれない</li>
		  <li>コーディングが必要な宿題もある</li>
		  <li>フォーラムで先生や他の受講生に質問を投げられる</li>
		  <li>全講義（だいたい6〜8週）を受講し、規定以上の評価なら修了証がもらえる</li>
		</ul>
      </article>

      <article class='fill'><img src="images/kininaru.jpg" width="100%"></article>

      <article>
        <h3>おすすめ講義</h3>
        <div class="build">
		  <p><b><a href="https://www.coursera.org/course/ml">Machine Learning</a></b> (Andrew Ng)<br>
			<ul>
			  <li>PRMLの基礎固めに最適（皆さんには易しすぎるかも）<br>
				プログラミング演習ではOctaveを使います<br>
				（次回8/20開講予定；全10週）</li>
			</ul>
		  </p>
		  <p><b><a href="https://www.coursera.org/course/pgm">Probabilistic Graphical Models</a></b> (Daphne Koller)<br>
			<ul>
			  <li>PRML8章（下巻）でやります<br>
				（次回9/24開講予定；全11週）</li>
			</ul>
		  </p>
		  <p><b><a href="https://www.coursera.org/course/nlp">Natural Language Processing</a></b> (Dan Jurafsky, Christopher Manning)<br>
			<ul>
			  <li>自然言語処理に興味のある方は是非！<br>
				（次回開講日未定；全8週）</li>
			</ul>
		  </p>
		</div>
	  </article>


      <article>
        <h2>さて本題</h2>
      </article>


      <article>
        <h3>パーセプトロン<br>
		<i>perceptron</i></h3>
        <div>
          <p>視覚と脳の機能をモデル化した線形分離アルゴリズム。</p>
		  <p>Frank Rosenblatt (1928-1969) が50年以上前 (1958) に提案。
			ニューラルネットワーク研究の礎として、パターン認識アルゴリズムの歴史の中で重要な地位を占めてきた。</p>
		  <p>当初は電動可変抵抗や電気モーターを利用したアナログハードウェアとして実装され、簡単な形や文字を識別するための学習などに用いられた。</p>
		  <p>単純なパーセプトロンは線形分離不可能な問題を解けないが、多層化し誤差逆伝播を行うことで線形分離不可能な問題にも適用できるようになった。</p>
		</div>
      </article>

      <article>
        <h3>パーセプトロンの仕組み</h3>
        <div>
		  <p>入力データ <img src="images/xn.png" valign="center">
		  （に非線形変換
		  <img src="images/phi.png" valign="center"> を適用した特徴量
		  <img src="images/phi_xn.png" valign="top">）に対し、
		  <img src="images/wT_phi_xn.png" valign="top"> の値が
		  クラスC<sub>1</sub>なら正、C<sub>2</sub> なら負になるように最適な重みベクトル
		  <img src="images/w.png" valign="center"> を学習する。</p>
		  <p>誤分類されるパターンが多ければ大きく、少なければ 0 に近づくような誤差関数を設定し、
			確率的勾配降下法によって <img src="images/w.png" valign="center"> を求める。</p>
		</div>
      </article>

      <article>
        <h3>パーセプトロン規準<br>
		<i>perceptron criterion</i></h3>
        <div>
		  <p>誤識別したパターンの総数を誤差関数とすれば自然だが、
			誤差が <img src="images/w.png" valign="center"> の区分的な定数関数であり、
			勾配がほとんどの場合 0 となってしまうため勾配降下法が効かない。</p>
		  <p>そこで別の誤差関数を考えたい。</p>
		  <p>目標変数 <img src="images/t_n.png" valign="center"> の値を { -1, +1 } とすれば、
			<img src="images/wT_phi_xn_tn.png" valign="top"> は
			正しく分類される場合に正、誤分類なら負の値を取る。</p>
		  <p>誤分類されたサンプル全てを含み、正しく分類されたサンプルは含まない集合
			<img src="images/cal_M.png" valign="center"> について
			<blockquote>
			  <img src="images/eq_4_54.png" valign="top">
			</blockquote>
		  </p>
		  <p>を<b>パーセプトロン規準</b>として定義する。</p>
		</div>
	  </article>

      <article>
        <h3>パーセプトロン規準（続き）<br>
		<i>perceptron criterion</i></h3>
        <div>
		  <p>これを最小化する <img src="images/w.png" valign="center"> は確率的勾配降下法で求めることができる。
			<blockquote>
			  <img src="images/eq_4_55a.png" valign="top">
			</blockquote>
		  </p>
		  <p><img src="images/w.png" valign="center"> に定数を掛けても
			<img src="images/wT_phi_xn.png" valign="top"> の正負は変化しないので、一般性を失うことなく
			学習率パラメータ <img src="images/eta.png" valign="center"> を 1 に設定することができる。</p>
		  <p>とかPRMLはしれっと言ってるんだけど、一般性を失うことなく云々って何？ 学習率の設定によっては収束速度とか変わりそうなイメージがあるけどどうなの？</p>
		</div>
      </article>

      <article>
        <h3>パーセプトロンの収束と学習率パラメータ</h3>
        <div>
		  <p>→sleepy_yoshiさんとのshuyoさんとの議論が参考になります：<br>
			<ul>
			  <li><a href="http://d.hatena.ne.jp/sleepy_yoshi/20120605/p1">パーセプトロンの収束性定理と学習率について</a></li>
			  <li><a href="http://d.hatena.ne.jp/sleepy_yoshi/20120616/p1">パーセプトロンの収束に学習率は関係ない件</a></li>
			  <li><a href="http://d.hatena.ne.jp/sleepy_yoshi/20120620/p1">パーセプトロンの収束に学習率が関係ないのは初期値が0のときだけ</a></li>
			</ul>
		  </p>
		  <p>結論から言うと
			<ul>
			  <li>重みベクトル <img src="images/w.png" valign="center"> の初期値が 0 の場合は <img src="images/eta.png" valign="center"> は収束速度に全く関係しない（同じ分離超平面に収束）</li>
			  <li>0 でない場合は収束までの誤り回数に影響する（学習率が小さい方がいい、とも一概に言えない）</li>
			</ul>
		  </p>
		</div>
      </article>

      <article class="smaller">
        <h4>[メモ] 確率的勾配降下法<br>
		  <i>stochastic gradient descent</i></h4>
		<div>
		  <p>3.1.3 (pp.141-142) 参照</p>
		  <p>個人的には大規模すぎるデータをシャッフルして（バッチではなく）１つずつ扱うのを「確率的 (stochastic) 勾配降下法」、次々現れるデータをオンライン的に扱うようなよくあるケースは「逐次的 (sequential) 勾配降下法」と呼ぶのがしっくりくる</p>
		  <p>あと勾配降下法 (gradient descent) / 最急降下法 (steepest descent) では個人的には勾配降下派だけれど</p>
		  <p>まあどっちでもいい</p>
		</div>
      </article>

       <article>
		<h3>パーセプトロン学習の各ステップ</h3>
		<p>１回１回の更新には、誤分類されたパターンからの誤差への寄与を減少させる効果がある。
		  <blockquote>
			<img src="images/eq_4_56a.png" valign="top"><br>
			　 　 　 　 　 　<img src="images/eq_4_56b.png" valign="top">
		  </blockquote>
		</p>
		<p>学習中の <img src="images/w.png" valign="center"> の変化によって、以前正しく分類されていたパターンを誤分類させるようなこともあり得る。</p>
		<p>パーセプトロン学習規則は、各ステップで総誤差関数を減少させることを保証していない。</p>
	  </article>

      <article>
        <h3>パーセプトロンの収束定理<br>
		  <i>perceptron convergence theorem</i></h3>
        <div>
		  <p>しかし、厳密解が存在する場合（＝学習データ集合が線形分離可能な場合）には、このアルゴリズムは有限回の繰り返しで厳密解に収束することが保証される。</p>
		  <p>これが<b>パーセプトロンの収束定理</b></p>
		  <p>とはいえ収束に必要な「有限回」はかなり多いので、実用的には分類不能なのか収束が遅いのか収束するまでわからない。</p>
		</div>
      </article>

      <article>
        <h3>パーセプトロンの限界</h3>
		<ul>
		  <li>確率的な出力を提供しない</li>
		  <li>K > 2クラスの場合への一般化が容易ではない</li>
		  <li><b>線形分離不可能なデータに対して収束しない</b><br>
			→ XORも解けないとはパターン認識四天王の面汚しよ<br>
			→ それ多層化して<ruby>誤差逆伝播法<rt>バックプロパゲーション</rt></ruby>を導入すれば出来るよ</li>
		</ul>
      </article>

      <article>
		<h3>実装例 (Octave)</h3>
<section>
<pre>
% データ読み込み
data = load('fig47dat.txt');
x = data(:, 1:2);
y = data(:, 3);
% w を適当に初期化
w = rand(1,2) * 0.001;
% η:学習率パラメータ
eta = 1;
% 確率的勾配降下法
maxiter = 100;
for t = 1:maxiter
  plotData(x, y, w, t);
  s = (x * w&#145;) .* y;
  bad = find(s &lt; 0);
  if length(bad) == 0
    break;
  end
  w += sum((x(bad,:) .* [y(bad) y(bad)]) * eta);
end
</pre>
</section>
	  </article>

      <article>
<section>
<pre>
function plotData(x, y, w, t)
  a = w(1,1); b = w(1,2);
  figure(t); clf(); hold on;
  title(sprintf('iter #%d', t));
  axis([-1 1 -1 1]);
  pos = find(y > 0); plot(x(pos,1), x(pos,2), '+b', 'markersize', 10);
  neg = find(y < 0); plot(x(neg,1), x(neg,2), '+r', 'markersize', 10);
  % 誤分類されたものに◯
  s = (x * w&#145;) .* y;
  bad = find(s < 0);
  plot(x(bad,1), x(bad,2), 'og', 'markersize', 20);
  % 決定境界に線を引く: ax + by = 0
  if abs(a) <= abs(b)
    plot([-1 1], [a/b -a/b], '-k', 'LineWidth', 1);
  else
    plot([b/a -b/a], [-1 1], '-k', 'LineWidth', 1);
  end
  hold off;
end
</pre>
</section>
      </article>

	  <article>
		<p>※データ (fig47dat.txt) は図4.7を見て適当に作成したもの</p>
<pre>
-0.75  0.9     1
-0.77  0.6     1
-0.53  0.65    1
-0.4   0.75    1
-0.1   -0.25   1
0.25   0.75   -1
0.4    -0.5   -1
0.45   0.3    -1
0.65   -0.9   -1
0.95   0.9    -1
</pre>
	  </article>

      <article class='fill'><img src="images/iter1.png"></article>
      <article class='fill'><img src="images/iter2.png"></article>
      <article class='fill'><img src="images/iter3.png"></article>
      <article class='fill'><img src="images/iter4.png"></article>

      <article>
        <h3>注意点</h3>
		<ul>
		  <li>この実装では誤分類サンプル数が 0 になるか、勾配降下を100ステップ行うかのどちらかで停止するようにしています</li>
		  <li>パーセプトロン学習アルゴリズムの性質上、パラメータの初期値やデータ入力順によってさまざまな解に収束します</li>
		  <li>紙面の都合により <code>plotData()</code> 関数に w = [0 0] 等を渡した場合のチェックを省いています。（完全なコードは発表資料と一緒に <a href="https://github.com/naoyat/slides/tree/master/prml/4.1.7">github</a> に上げてあります）</li>
		  <li>時間があれば収束しないケースのデモでもやります。</li>
		</ul>
	  </article>

      <article>
        <h3>References</h3>
        <ul>
		  <li>発表資料<br>
			<a href="http://naoyat.github.com/slides/prml/4.1.7/slide.html">http://naoyat.github.com/slides/prml/4.1.7/slide.html</a></li>
		  <li>実装コード (Octave) + サンプルデータ<br>
			<a href="https://github.com/naoyat/slides/tree/master/prml/4.1.7">https://github.com/naoyat/slides/tree/master/prml/4.1.7</a></li>
          <li>naoya_t@hatenablog<br>
			<a href="http://naoyat.hatenablog.jp/">http://naoyat.hatenablog.jp/</a></li>
		  <li>Google HTML5 slide template<br>
			<a href="http://code.google.com/p/html5slides/">http://code.google.com/p/html5slides/</a></li>
		  <li>Coursera<br>
			<a href="https://www.coursera.org/">https://www.coursera.org/</a></li>
		  <li>(TED Talks) Daphne Koller: What we're learning from online education<br>
			<a href="http://www.ted.com/talks/daphne_koller_what_we_re_learning_from_online_education.html">http://www.ted.com/talks/daphne_koller_what_we_re_
			  learning_from_online_education.html</a></li>
        </ul>
      </article>

    </section>

  </body>
</html>

